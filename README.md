# finetune_gptj_8_bit
The repo has source code to fine tune gptj using 8 bit quantization so that it fits in single GPU and consumes around 14GB of memory
